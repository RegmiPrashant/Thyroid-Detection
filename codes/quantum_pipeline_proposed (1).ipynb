{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG2zwfJ6vmFw",
        "outputId": "cfef2cca-578c-45b2-a9de-d6b8270c88a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Collecting tomlkit (from pennylane)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.40 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.40->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp311-cp311-manylinux_2_28_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, tomlkit, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.1 diastatic-malt-2.15.2 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\n"
          ]
        }
      ],
      "source": [
        "pip install pennylane\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk2Qx-IGvwxt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pennylane as qml\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "# Define constants\n",
        "PATCH_SIZE = 2  # 2x2 patches\n",
        "N_QUBITS = 4    # 4 qubits (one per pixel in a patch)\n",
        "N_QUANTUM_LAYERS = 2  # Number of variational layers\n",
        "\n",
        "# Create a quantum device\n",
        "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
        "\n",
        "# Define the parameterized quantum circuit for feature extraction\n",
        "@qml.qnode(dev)\n",
        "def quantum_circuit(patch_data, weights):\n",
        "    \"\"\"\n",
        "    Quantum circuit that encodes patch data and applies parameterized operations\n",
        "\n",
        "    Args:\n",
        "        patch_data: Array of 4 pixel values from a 2x2 patch\n",
        "        weights: Trainable weights for the variational quantum circuit\n",
        "\n",
        "    Returns:\n",
        "        Expectation values of different observables\n",
        "    \"\"\"\n",
        "    # Data encoding layer\n",
        "    for i in range(N_QUBITS):\n",
        "        # Map pixel value to [0, π]\n",
        "        qml.RX(patch_data[i] * np.pi, wires=i)\n",
        "\n",
        "    # Variational quantum layers\n",
        "    for layer in range(N_QUANTUM_LAYERS):\n",
        "        # Rotation gates with trainable parameters\n",
        "        for i in range(N_QUBITS):\n",
        "            qml.RX(weights[layer, i, 0], wires=i)\n",
        "            qml.RY(weights[layer, i, 1], wires=i)\n",
        "            qml.RZ(weights[layer, i, 2], wires=i)\n",
        "\n",
        "        # Entanglement layer - create a ring of entanglement\n",
        "        for i in range(N_QUBITS):\n",
        "            qml.CNOT(wires=[i, (i + 1) % N_QUBITS])\n",
        "\n",
        "    # Measure different observables for richer feature extraction\n",
        "    return [\n",
        "        qml.expval(qml.PauliZ(0)),\n",
        "        qml.expval(qml.PauliX(1)),\n",
        "        qml.expval(qml.PauliY(2)),\n",
        "        qml.expval(qml.PauliZ(3))\n",
        "    ]\n",
        "\n",
        "# Initialize random weights for the quantum circuit\n",
        "def init_quantum_weights():\n",
        "    # Each qubit has 3 rotation parameters (RX, RY, RZ) for each layer\n",
        "    return np.random.uniform(\n",
        "        low=0, high=2*np.pi,\n",
        "        size=(N_QUANTUM_LAYERS, N_QUBITS, 3)\n",
        "    )\n",
        "\n",
        "def quantum_filter_transform(image, weights):\n",
        "    \"\"\"\n",
        "    Transform an image using quantum filters\n",
        "\n",
        "    Args:\n",
        "        image: Input image (grayscale, already normalized to [0,1])\n",
        "        weights: Weights for the quantum circuit\n",
        "\n",
        "    Returns:\n",
        "        Quantum features extracted from the image\n",
        "    \"\"\"\n",
        "    h, w = image.shape\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    h_out = h // PATCH_SIZE\n",
        "    w_out = w // PATCH_SIZE\n",
        "\n",
        "    # Initialize output tensor (4 features per patch)\n",
        "    quantum_features = np.zeros((h_out, w_out, 4))\n",
        "\n",
        "    # Process each patch\n",
        "    for i in range(0, h - PATCH_SIZE + 1, PATCH_SIZE):\n",
        "        for j in range(0, w - PATCH_SIZE + 1, PATCH_SIZE):\n",
        "            # Extract patch\n",
        "            patch = image[i:i+PATCH_SIZE, j:j+PATCH_SIZE]\n",
        "\n",
        "            # Skip patches that aren't full size\n",
        "            if patch.shape != (PATCH_SIZE, PATCH_SIZE):\n",
        "                continue\n",
        "\n",
        "            # Flatten the patch\n",
        "            patch_flat = patch.flatten()\n",
        "\n",
        "            # Apply quantum circuit\n",
        "            features = quantum_circuit(patch_flat, weights)\n",
        "\n",
        "            # Store features\n",
        "            quantum_features[i // PATCH_SIZE, j // PATCH_SIZE, :] = features\n",
        "\n",
        "    return quantum_features\n",
        "\n",
        "def save_checkpoint(quantum_features, last_processed_idx, checkpoint_path):\n",
        "    \"\"\"Save checkpoint with processed features and last processed index\"\"\"\n",
        "    checkpoint = {\n",
        "        'quantum_features': quantum_features,\n",
        "        'last_processed_idx': last_processed_idx\n",
        "    }\n",
        "    with open(checkpoint_path, 'wb') as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "    print(f\"Checkpoint saved at index {last_processed_idx}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_path):\n",
        "    \"\"\"Load checkpoint with processed features and last processed index\"\"\"\n",
        "    with open(checkpoint_path, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "    print(f\"Checkpoint loaded. Resuming from index {checkpoint['last_processed_idx'] + 1}\")\n",
        "    return checkpoint['quantum_features'], checkpoint['last_processed_idx']\n",
        "\n",
        "def process_dataset(x_data, quantum_weights, batch_size=10,\n",
        "                    save_dir=\"/content/drive/MyDrive/quantum_features\",\n",
        "                    start_idx=0, checkpoint_interval=20):\n",
        "    \"\"\"\n",
        "    Process an entire dataset using quantum circuits with checkpointing\n",
        "\n",
        "    Args:\n",
        "        x_data: Array of images [n_samples, height, width, channels]\n",
        "        quantum_weights: Weights for the quantum circuit\n",
        "        batch_size: Number of images to process at once (for memory efficiency)\n",
        "        save_dir: Directory to save the processed features\n",
        "        start_idx: Index to start processing from (for resuming)\n",
        "        checkpoint_interval: How often to save checkpoints (number of samples)\n",
        "\n",
        "    Returns:\n",
        "        Path to the saved quantum features\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Define checkpoint path\n",
        "    checkpoint_path = os.path.join(save_dir, \"processing_checkpoint.pkl\")\n",
        "\n",
        "    # Normalize images to [0,1]\n",
        "    x_data = x_data.astype(np.float32) / 255.0\n",
        "\n",
        "    # Get dimensions\n",
        "    n_samples = x_data.shape[0]\n",
        "    h, w = x_data.shape[1], x_data.shape[2]\n",
        "    h_out = h // PATCH_SIZE\n",
        "    w_out = w // PATCH_SIZE\n",
        "\n",
        "    # Check if we're resuming from a checkpoint\n",
        "    if os.path.exists(checkpoint_path) and start_idx == 0:\n",
        "        quantum_features, last_idx = load_checkpoint(checkpoint_path)\n",
        "        start_idx = last_idx + 1\n",
        "    else:\n",
        "        # Initialize output array\n",
        "        quantum_features = np.zeros((n_samples, h_out, w_out, 4))\n",
        "        last_idx = start_idx - 1\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(start_idx, n_samples, batch_size):\n",
        "        end_idx = min(i + batch_size, n_samples)\n",
        "        print(f\"Processing images {i} to {end_idx-1} of {n_samples}...\")\n",
        "\n",
        "        for j in range(i, end_idx):\n",
        "            # For grayscale images\n",
        "            img = x_data[j, :, :, 0]\n",
        "            quantum_features[j] = quantum_filter_transform(img, quantum_weights)\n",
        "\n",
        "            # Print progress\n",
        "            if (j + 1) % 10 == 0:\n",
        "                print(f\"  Processed image {j+1}/{n_samples}\")\n",
        "\n",
        "            # Save checkpoint at intervals\n",
        "            if (j + 1) % checkpoint_interval == 0 or j == end_idx - 1:\n",
        "                save_checkpoint(quantum_features, j, checkpoint_path)\n",
        "\n",
        "    # Save the final quantum features\n",
        "    features_path = os.path.join(save_dir, \"quantum_features.npy\")\n",
        "    np.save(features_path, quantum_features)\n",
        "\n",
        "    # Save the quantum weights for reproducibility\n",
        "    weights_path = os.path.join(save_dir, \"quantum_weights.npy\")\n",
        "    np.save(weights_path, quantum_weights)\n",
        "\n",
        "    # Remove the checkpoint file since we've completed processing\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        os.remove(checkpoint_path)\n",
        "\n",
        "    print(f\"Quantum features saved to {features_path}\")\n",
        "    print(f\"Quantum weights saved to {weights_path}\")\n",
        "\n",
        "    return features_path\n",
        "\n",
        "def load_xray_dataset():\n",
        "    DATASET_DIR = '/content/drive/MyDrive/quantum_thyroid/sorted'\n",
        "    # Image Size & Quantum Patch Size\n",
        "    IMG_SIZE = (224, 224)\n",
        "    classes = ['0','2','3','4a','4b','4c','5']\n",
        "    images, labels = [],[]\n",
        "    for label in classes:\n",
        "        class_dir = os.path.join(DATASET_DIR, str(label))\n",
        "        for file in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, file)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.resize(img, IMG_SIZE) / 255.0\n",
        "            img = np.stack([img] * 3, axis=-1)\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def split_and_save_data(x_data, y_data, save_dir=\"/content/drive/MyDrive/data_splits\"):\n",
        "    \"\"\"Split data into train/test and save the indices\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Split the data\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        np.arange(len(x_data)), y_data, test_size=0.2, random_state=42, stratify=y_data\n",
        "    )\n",
        "\n",
        "    # Save the splits\n",
        "    splits = {\n",
        "        'train_indices': x_train,\n",
        "        'test_indices': x_test,\n",
        "        'train_labels': y_train,\n",
        "        'test_labels': y_test\n",
        "    }\n",
        "\n",
        "    split_path = os.path.join(save_dir, \"data_splits.pkl\")\n",
        "    with open(split_path, 'wb') as f:\n",
        "        pickle.dump(splits, f)\n",
        "\n",
        "    print(f\"Data splits saved to {split_path}\")\n",
        "    print(f\"Train set: {len(x_train)} samples\")\n",
        "    print(f\"Test set: {len(x_test)} samples\")\n",
        "\n",
        "    return splits\n",
        "\n",
        "def load_or_init_weights(weights_path):\n",
        "    \"\"\"Load existing weights or initialize new ones\"\"\"\n",
        "    if os.path.exists(weights_path):\n",
        "        print(f\"Loading existing quantum weights from {weights_path}\")\n",
        "        return np.load(weights_path)\n",
        "    else:\n",
        "        print(\"Initializing new quantum weights...\")\n",
        "        return init_quantum_weights()\n",
        "\n",
        "def main(data_dir=\"./xray_data\", resize=(224, 224), batch_size=10,\n",
        "         output_dir=\"/content/drive/MyDrive/quantum_xray_results\",\n",
        "         resume=True, start_idx=0, checkpoint_interval=50):\n",
        "    \"\"\"Main function to run the quantum preprocessing pipeline with resume capability\"\"\"\n",
        "    # Make sure the base output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for features and splits\n",
        "    quantum_features_dir = os.path.join(output_dir, \"quantum_features\")\n",
        "    data_splits_dir = os.path.join(output_dir, \"data_splits\")\n",
        "    os.makedirs(quantum_features_dir, exist_ok=True)\n",
        "    os.makedirs(data_splits_dir, exist_ok=True)\n",
        "\n",
        "    # Define weights path\n",
        "    weights_path = os.path.join(quantum_features_dir, \"quantum_weights.npy\")\n",
        "\n",
        "    # Check if we need to load the dataset\n",
        "    checkpoint_path = os.path.join(quantum_features_dir, \"processing_checkpoint.pkl\")\n",
        "    processing_needed = True\n",
        "\n",
        "    if resume and os.path.exists(checkpoint_path):\n",
        "        print(\"Checkpoint found. Resuming from previous run.\")\n",
        "\n",
        "        # Load the dataset\n",
        "        x_data, y_data = load_xray_dataset()\n",
        "\n",
        "        # Load existing weights or initialize new ones\n",
        "        quantum_weights = load_or_init_weights(weights_path)\n",
        "\n",
        "        # Check if data splits exist, create if not\n",
        "        split_path = os.path.join(data_splits_dir, \"data_splits.pkl\")\n",
        "        if os.path.exists(split_path):\n",
        "            print(f\"Loading existing data splits from {split_path}\")\n",
        "            with open(split_path, 'rb') as f:\n",
        "                splits = pickle.load(f)\n",
        "        else:\n",
        "            # Split and save the data indices\n",
        "            splits = split_and_save_data(x_data, y_data, save_dir=data_splits_dir)\n",
        "    else:\n",
        "        # Starting from scratch\n",
        "        print(\"Starting new processing run...\")\n",
        "\n",
        "        # Load the dataset\n",
        "        x_data, y_data = load_xray_dataset()\n",
        "\n",
        "        # Split and save the data indices\n",
        "        splits = split_and_save_data(x_data, y_data, save_dir=data_splits_dir)\n",
        "\n",
        "        # Initialize or load quantum weights\n",
        "        quantum_weights = load_or_init_weights(weights_path)\n",
        "\n",
        "    # Process the dataset\n",
        "    print(\"Starting quantum feature extraction...\")\n",
        "    features_path = process_dataset(x_data, quantum_weights,\n",
        "                                   batch_size=batch_size,\n",
        "                                   save_dir=quantum_features_dir,\n",
        "                                   start_idx=start_idx,\n",
        "                                   checkpoint_interval=checkpoint_interval)\n",
        "\n",
        "    # Save a summary file with information about the processing\n",
        "    summary_path = os.path.join(output_dir, \"processing_summary.txt\")\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(f\"Quantum preprocessing completed on {os.path.basename(__file__)}\\n\")\n",
        "        f.write(f\"Number of samples processed: {len(x_data)}\\n\")\n",
        "        f.write(f\"Image resize dimensions: {resize}\\n\")\n",
        "        f.write(f\"Patch size: {PATCH_SIZE}x{PATCH_SIZE}\\n\")\n",
        "        f.write(f\"Number of qubits: {N_QUBITS}\\n\")\n",
        "        f.write(f\"Number of quantum layers: {N_QUANTUM_LAYERS}\\n\")\n",
        "        f.write(f\"Quantum features saved to: {features_path}\\n\")\n",
        "        f.write(f\"Data splits saved to: {data_splits_dir}/data_splits.pkl\\n\")\n",
        "\n",
        "    print(\"\\nQuantum preprocessing complete!\")\n",
        "    print(f\"All results saved to: {output_dir}\")\n",
        "    print(f\"You can now run the classification model using the generated quantum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y53zHwCOdMOy",
        "outputId": "30d036c8-c883-41db-f819-7b0f0af9f9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint found. Resuming from previous run.\n",
            "Initializing new quantum weights...\n",
            "Loading existing data splits from /content/drive/MyDrive/quantum_xray_results/data_splits/data_splits.pkl\n",
            "Starting quantum feature extraction...\n",
            "Checkpoint loaded. Resuming from index 650\n",
            "Processing images 650 to 659 of 823...\n",
            "  Processed image 660/823\n",
            "Checkpoint saved at index 659\n",
            "Processing images 660 to 669 of 823...\n",
            "  Processed image 670/823\n",
            "Checkpoint saved at index 669\n",
            "Processing images 670 to 679 of 823...\n",
            "  Processed image 680/823\n",
            "Checkpoint saved at index 679\n",
            "Processing images 680 to 689 of 823...\n",
            "  Processed image 690/823\n",
            "Checkpoint saved at index 689\n",
            "Processing images 690 to 699 of 823...\n",
            "  Processed image 700/823\n",
            "Checkpoint saved at index 699\n",
            "Processing images 700 to 709 of 823...\n",
            "  Processed image 710/823\n",
            "Checkpoint saved at index 709\n",
            "Processing images 710 to 719 of 823...\n",
            "  Processed image 720/823\n",
            "Checkpoint saved at index 719\n",
            "Processing images 720 to 729 of 823...\n",
            "  Processed image 730/823\n",
            "Checkpoint saved at index 729\n",
            "Processing images 730 to 739 of 823...\n",
            "  Processed image 740/823\n",
            "Checkpoint saved at index 739\n",
            "Processing images 740 to 749 of 823...\n",
            "  Processed image 750/823\n",
            "Checkpoint saved at index 749\n",
            "Processing images 750 to 759 of 823...\n",
            "  Processed image 760/823\n",
            "Checkpoint saved at index 759\n",
            "Processing images 760 to 769 of 823...\n",
            "  Processed image 770/823\n",
            "Checkpoint saved at index 769\n",
            "Processing images 770 to 779 of 823...\n",
            "  Processed image 780/823\n",
            "Checkpoint saved at index 779\n",
            "Processing images 780 to 789 of 823...\n",
            "  Processed image 790/823\n",
            "Checkpoint saved at index 789\n",
            "Processing images 790 to 799 of 823...\n",
            "  Processed image 800/823\n",
            "Checkpoint saved at index 799\n",
            "Processing images 800 to 809 of 823...\n",
            "  Processed image 810/823\n",
            "Checkpoint saved at index 809\n",
            "Processing images 810 to 819 of 823...\n",
            "  Processed image 820/823\n",
            "Checkpoint saved at index 819\n",
            "Processing images 820 to 822 of 823...\n",
            "Checkpoint saved at index 822\n",
            "Quantum features saved to /content/drive/MyDrive/quantum_xray_results/quantum_features/quantum_features.npy\n",
            "Quantum weights saved to /content/drive/MyDrive/quantum_xray_results/quantum_features/quantum_weights.npy\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7d99aaa5428b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive.mount('/content/drive')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-e949a5eb5f35>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, resize, batch_size, output_dir, resume, start_idx, checkpoint_interval)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0msummary_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"processing_summary.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Quantum preprocessing completed on {os.path.basename(__file__)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of samples processed: {len(x_data)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image resize dimensions: {resize}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # First, check if Google Drive is mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive is not mounted. Please run the following code first:\")\n",
        "        print(\"from google.colab import drive\")\n",
        "        print(\"drive.mount('/content/drive')\")\n",
        "    else:\n",
        "        main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJbSLJpFdnlS"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}